{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9c9466",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import spacy\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import pdfplumber\n",
    "import spacy\n",
    "import string\n",
    "import math\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "from sklearn.metrics import recall_score\n",
    "import pandas as pd\n",
    "nlp = spacy.load(\"ru_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d54c46c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Грузим весь текст\n",
    "def get_text_from_pdf(pdf_path: str) -> dict:\n",
    "    # Берёт текст с файла в папке\n",
    "    pdf_text = \"\"\n",
    "    page_number = {} # хранит словарь вида номер_страницы:айди последнего элемента\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        n_pages = len(pdf.pages)\n",
    "        for page_id in range(n_pages):\n",
    "            page_text = pdf.pages[page_id].extract_text().replace(\"\\n\", \" \")\n",
    "            page_number[page_id] = page_number.get(page_id-1, 0) + len(page_text)\n",
    "            pdf_text += page_text\n",
    "    return pdf_text, page_number\n",
    "\n",
    "def get_loaded_pdf(pdf):\n",
    "    pdf_text = \"\"\n",
    "    page_number = {} # хранит словарь вида номер_страницы:айди последнего элемента\n",
    "    n_pages =len(pdf.pages) \n",
    "    for page_id in range(n_pages):\n",
    "        page_text = pdf.pages[page_id].extract_text().replace(\"\\n\", \" \")\n",
    "        page_number[page_id] = page_number.get(page_id-1, 0) + len(page_text)\n",
    "        pdf_text += page_text\n",
    "    return pdf_text, page_number\n",
    "\n",
    "def get_page_id(symbol_id:int, page_number:dict) -> int:\n",
    "    for page_id in range(len(page_number)):\n",
    "        if symbol_id <= page_number[page_id]:\n",
    "            return page_id\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c255fb6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Версия создания финального df с NER. Будем накладывать NER после первого преобразования\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "def get_nearest_similarities(text: str, standard_sub_text: str) -> list[str]:\n",
    "    text_len = len(text)\n",
    "    window_len = len(standard_sub_text) + 1\n",
    "    sub_texts = []\n",
    "    for i in range(text_len):\n",
    "        sub_texts.append(text[i:i+window_len])\n",
    "    tfidf = TfidfVectorizer()\n",
    "    mx_tf = tfidf.fit_transform(sub_texts)\n",
    "    entry = tfidf.transform([standard_sub_text])\n",
    "    cosine_similarities = linear_kernel(entry, mx_tf).flatten()\n",
    "    df = pd.DataFrame({'texts': sub_texts})\n",
    "    df['cos_similarities'] = cosine_similarities\n",
    "    df = df[df.cos_similarities > 0]\n",
    "    #df['ner_similarities'] = df.texts.apply(lambda text: nlp(text).similarity(nlp(standard_sub_text)))\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Преобразуем в удобный вид\n",
    "def get_final_df(df, reference_seq, page_number, whole_text):\n",
    "    final_df = df.copy()\n",
    "\n",
    "    final_df[\"starts_at\"] = final_df.index\n",
    "    final_df[\"ends_at\"] = final_df[\"starts_at\"] + final_df[\"texts\"].apply(lambda text: len(text))\n",
    "\n",
    "    final_df[\"start_page_id\"] = final_df.starts_at.apply(lambda symbol_id: get_page_id(symbol_id, page_number) + 1)\n",
    "    final_df[\"end_page_id\"] = final_df.ends_at.apply(lambda symbol_id: get_page_id(symbol_id, page_number) + 1)\n",
    "\n",
    "    final_df = final_df[final_df[\"cos_similarities\"] > THRESHOLD]\n",
    "    \n",
    "    final_df = final_df.reset_index()\n",
    "    final_df = final_df[final_df['index'] + len(reference_seq) < final_df['index'].shift(-1)]\n",
    "    \n",
    "    docs = list(nlp.pipe(final_df['texts']))\n",
    "    standard_doc = nlp(reference_seq)\n",
    "    final_df['ner_similarities'] = [doc.similarity(standard_doc) for doc in docs]\n",
    "        \n",
    "    #  Ищем откуда начинается фраза в текста\n",
    "    padding = max(10, int(len(reference_seq) * 0.4))\n",
    "    window_overlap = max(min(15, int(len(reference_seq) * 0.1)), 1)\n",
    "    for ind in final_df[\"index\"]:\n",
    "        min_new = max(0, ind - padding)\n",
    "        max_new = min(len(whole_text), ind + padding)\n",
    "        best_id = ind\n",
    "        best_sim = 0\n",
    "        f_skip = 0\n",
    "        for i in range(min_new, max_new):\n",
    "            # Сначала пробуем найти полное совпадение в начале эталонного текста\n",
    "            #print(whole_text[i:i+window_overlap].lower())\n",
    "            if reference_seq.lower().startswith(whole_text[i:i+window_overlap].lower()):\n",
    "                best_sim = 1\n",
    "                best_id = i\n",
    "                f_skip = 1\n",
    "                \n",
    "        if not f_skip:\n",
    "            # Если не получилось поймать полное совпадение, ищем лучшее наложение\n",
    "            for i in range(min_new, max_new):\n",
    "                sentence = whole_text[i:i+len(reference_seq) + 1].lower()\n",
    "                similarity = nlp(sentence).similarity(nlp(reference_seq.lower()))\n",
    "                if similarity > best_sim:\n",
    "                    best_sim = similarity\n",
    "                    best_id = i\n",
    "        final_df.loc[final_df[\"index\"] == ind, \"cos_similarities\"] = best_sim\n",
    "        final_df.loc[final_df[\"index\"] == ind, \"texts\"] = whole_text[best_id:best_id+len(reference_seq) + 1]\n",
    "        final_df.loc[final_df[\"index\"] == ind, \"index\"]= best_id  \n",
    "    final_df[\"mask\"] = final_df[\"texts\"].apply(lambda x: get_diff_mask(reference_seq,x ))\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# reference_seq = \"\"\"ВТОРОЙ ПУТЬ НА ПЕРЕГОНЕ КИЗИР-ЖУРАВЛЕВО КРАСНОЯРСКОЙ ЖЕЛЕЗНОЙ ДОРОГИ\"\"\"\n",
    "# reference_seq = reference_seq.replace(\"\\n\", \" \")\n",
    "# fila_name=\"Кузб-183267_КРАС–ИЭИ1_изм.6.00256-21_КРЭ-26756\"\n",
    "# whole_text, page_number = get_text_from_pdf(f\"ПД для ии/{fila_name}.pdf\")\n",
    "\n",
    "# df = get_nearest_similarities(whole_text, reference_seq)\n",
    "\n",
    "# final_df = get_final_df(df, reference_seq, page_number, whole_text)\n",
    "# final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08c89831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ещё один вариант поиска расстояний\n",
    "\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# padding = int(len(reference_seq) * 0.25)\n",
    "# for ind in final_df[\"index\"][14:]:\n",
    "#     print(ind)\n",
    "#     min_new = max(0, ind - padding)\n",
    "#     max_new = min(len(whole_text), ind + padding)\n",
    "#     for i in range(min_new, max_new):\n",
    "#         sentence = whole_text[i:i+len(reference_seq) + 1].lower()\n",
    "#         tfidf = vectorizer.fit_transform([sentence, reference_seq.lower()])\n",
    "#         similarity = cosine_similarity(tfidf[0], tfidf[1])[0][0]\n",
    "#         print(similarity, i,sentence, reference_seq)\n",
    "#     break\n",
    "\n",
    "\n",
    "# # fit and transform the sentences\n",
    "# tfidf = vectorizer.fit_transform([sentence1, sentence2])\n",
    "\n",
    "# # calculate the cosine similarity between the sentences\n",
    "# similarity = cosine_similarity(tfidf[0], tfidf[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d6b0831",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# reference_seq = \"\"\"ПЕРВЫЙ ПУТЬ НА ПЕРЕГОНИ КИЗИР-ЖУРАВЛЕВО КРАСНОЯРСКОЙ ЖЕЛЕЗНОЙ ДОРОГИ\"\"\"\n",
    "# whole_text, page_number = get_text_from_pdf(\"ПД для ии/Кузб-183267_КРАС–ИЭИ1_изм.6.00256-21_КРЭ-26756.pdf\")\n",
    "\n",
    "# def get_nearest_similarities(text: str, standard_sub_text: str) -> list[str]:\n",
    "#     text_len = len(text)\n",
    "#     window_len = len(standard_sub_text)\n",
    "#     sub_texts = []\n",
    "#     for i in range(text_len):\n",
    "#         sub_texts.append(text[i:i+window_len])\n",
    "#     tfidf = TfidfVectorizer()\n",
    "#     mx_tf = tfidf.fit_transform(sub_texts)\n",
    "#     entry = tfidf.transform([standard_sub_text])\n",
    "#     cosine_similarities = linear_kernel(entry, mx_tf).flatten()\n",
    "#     df = pd.DataFrame({'texts': sub_texts})\n",
    "#     df['cos_similarities'] = cosine_similarities\n",
    "#     df = df[df.cos_similarities > 0]\n",
    "#     #df['ner_similarities'] = df.texts.apply(lambda text: nlp(text).similarity(nlp(standard_sub_text)))\n",
    "    \n",
    "#     return df\n",
    "\n",
    "#df = get_nearest_similarities(whole_text, reference_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748a5c5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Преобразуем в удобный вид\n",
    "\n",
    "# def get_final_df(df, reference_seq, page_number):\n",
    "#     class Cumulative:\n",
    "#         def __init__(self):\n",
    "#             self.val = 0\n",
    "#         def increase(self):\n",
    "#             self.val += 1\n",
    "#             return self.val\n",
    "\n",
    "#     cumulative = Cumulative()\n",
    "#     df[\"id\"] = df.index\n",
    "#     seq_len = len(reference_seq)\n",
    "#     df[\"seq\"] = df.id.apply(lambda x: x + 2*seq_len)\n",
    "#     df[\"seq_lagged\"] = df.seq.shift(1)\n",
    "#     df[\"seq_id\"] = (df.id > df.seq_lagged).apply(lambda x: cumulative.increase() if x else cumulative.val)\n",
    "#     seq_ids = df[[\"texts\", \"cos_similarities\", \"seq_id\"]].seq_id.unique()\n",
    "\n",
    "#     best_similarities = []\n",
    "#     for seq_id in seq_ids:\n",
    "#         seq_df = df[df.seq_id==seq_id]\n",
    "#         max_cos_seq_df = seq_df[seq_df.cos_similarities==seq_df.cos_similarities.max()]\n",
    "#         best_similarities.append(max_cos_seq_df[[\"texts\", \"cos_similarities\"]].head())\n",
    "\n",
    "#     final_df = pd.concat(best_similarities)\n",
    "\n",
    "#     final_df[\"starts_at\"] = final_df.index\n",
    "#     final_df[\"ends_at\"] = final_df[\"starts_at\"] + final_df[\"texts\"].apply(lambda text: len(text))\n",
    "\n",
    "#     final_df[\"start_page_id\"] = final_df.starts_at.apply(lambda symbol_id: get_page_id(symbol_id, page_number))\n",
    "#     final_df[\"end_page_id\"] = final_df.ends_at.apply(lambda symbol_id: get_page_id(symbol_id, page_number))\n",
    "\n",
    "# #     final_df = final_df[final_df[\"cos_similarities\"] > 0.9]\n",
    "#     return final_df\n",
    "\n",
    "# # final_df = get_final_df(df, reference_seq, page_number)\n",
    "# # final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b422ec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Нахождение различий между двумя текстами\n",
    "def find_difference(ref_text, file_text):\n",
    "    braces = [\"'\", '\"', \"«\", \"»\"]\n",
    "    punctuation_marks = list(string.punctuation)\n",
    "    \n",
    "    letter_mismatch = []\n",
    "    uppercase_mismatch = []\n",
    "    braces_mismatch = []\n",
    "    punctuation_mismatch = []\n",
    "    for i in range(len(ref_text)):\n",
    "        if ref_text[i] != file_text[i]:\n",
    "            if ref_text[i].lower() == file_text[i].lower():\n",
    "                uppercase_mismatch.append(i)    \n",
    "            elif ref_text[i] in braces and file_text[i] in braces:\n",
    "                braces_mismatch.append(i)\n",
    "            elif ref_text[i] in punctuation_marks and file_text[i] in punctuation_marks:\n",
    "                punctuation_mismatch.append(i)\n",
    "            else:\n",
    "                letter_mismatch.append(i)\n",
    "  \n",
    "    return_text = \"Были обнаружены ошибки видов: \"\n",
    "    if letter_mismatch:\n",
    "        return_text +=  f\"\"\"символьное несовпадение,\"\"\"\n",
    "    if uppercase_mismatch:  \n",
    "        return_text += f\"\"\" ошибка в регистре,\"\"\"\n",
    "    if braces_mismatch:\n",
    "        return_text += f\"\"\" неправильные скобки,\"\"\"\n",
    "    if punctuation_mismatch:\n",
    "        return_text += f\"\"\" ошибка в пунктуации,\"\"\"\n",
    "    return_text = return_text[:-1]\n",
    "    if not uppercase_mismatch and not letter_mismatch:\n",
    "        return_text = \"Ошибки отсутсвтуют\"\n",
    "\n",
    "    return return_text\n",
    "        \n",
    "#find_difference(\"ВТОРОЙ ПоТЬ НА ПЕРЕГОНЕ КИЗИР-ЖУРАВЛЕВО КРАСНОЯРСКОЙ ЖЕЛЕЗНОЙ ДОРОГИ\", \"Второй путь на перегоне Кизир-Журавлево Красноярской железной дороги\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73bb702d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_diff_mask(standard_text: str, matched_text: str) -> str:\n",
    "    mask_len = len(standard_text)\n",
    "    diff_mask = \"\".join([matched_text[char_id] if matched_text[char_id]==standard_text[char_id] else \"_\" for char_id in range(mask_len)])\n",
    "    return diff_mask\n",
    "\n",
    "def parse_model_results(df: pd.DataFrame, standard_text: str, k: float = 0.5) -> dict:\n",
    "    n_matches = df.shape[0]\n",
    "    matches_info = []\n",
    "    for match_id in range(n_matches):\n",
    "        matched_object = df.iloc[match_id]\n",
    "        match_info = {}\n",
    "        match_info[\"Найденное совпадение\"] = matched_object.texts\n",
    "        match_info[\"Отличия\"] = get_diff_mask(str(standard_text), str(matched_object.texts))\n",
    "        match_info[\"Степень сходства Cosine similarity\"] = matched_object.cos_similarities\n",
    "        match_info[\"Степень сходства Spacy matching\"] = matched_object.ner_similarities\n",
    "        match_info[\"Страницы\"] = [*set([matched_object.start_page_id, matched_object.end_page_id])]\n",
    "        matches_info.append(match_info)\n",
    "    return matches_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388e91af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:39: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "<timed exec>:62: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pdfplumber\n",
    "from PyQt5.QtWidgets import QApplication, QWidget, QVBoxLayout, QPushButton, QLineEdit, QLabel, QFileDialog, QTableWidget, QTableWidgetItem\n",
    "import nltk\n",
    "\n",
    "class MyWidget(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.initUI()\n",
    "\n",
    "    def initUI(self):\n",
    "        self.label = QLabel('Файлы:', self)\n",
    "        self.file_label = QLabel(self)\n",
    "        self.table = QTableWidget(self)\n",
    "        self.table.setColumnCount(6)\n",
    "        self.table.setHorizontalHeaderLabels(\n",
    "            ['Имя файла', 'Страница', 'Виды ошибок', 'Эталонный текст', 'Найденный текст', 'Маска'])\n",
    "        self.check_button = QPushButton('Проверить файлы', self)\n",
    "        self.check_button.clicked.connect(self.check_files)\n",
    "        self.remove_button = QPushButton('Убрать файлы', self)\n",
    "        self.remove_button.clicked.connect(self.remove_files)\n",
    "        self.search_input = QLineEdit(self)\n",
    "        \n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.search_input)\n",
    "        layout.addWidget(self.label)\n",
    "        layout.addWidget(self.file_label)\n",
    "        layout.addWidget(self.table)\n",
    "        layout.addWidget(self.check_button)\n",
    "        layout.addWidget(self.remove_button)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "\n",
    "        self.setStyleSheet('''\n",
    "            QWidget {\n",
    "                background-color: #eef8fa;\n",
    "                color: #6e6e6e;\n",
    "            }\n",
    "            QPushButton {\n",
    "                background-color: #ffe5df;\n",
    "                color: #6e6e6e;\n",
    "                border: none;\n",
    "                padding: 5px;\n",
    "                margin: 5px;\n",
    "            }\n",
    "            QLineEdit {\n",
    "                padding: 5px;\n",
    "                margin: 5px;\n",
    "            }\n",
    "            QLabel {\n",
    "                margin: 5px;\n",
    "            }\n",
    "            QTableWidget {\n",
    "                margin: 5px;\n",
    "            }\n",
    "        ''')\n",
    "        \n",
    "    def choose_files(self):\n",
    "        file_dialog = QFileDialog()\n",
    "        file_dialog.setFileMode(QFileDialog.ExistingFiles)\n",
    "        if file_dialog.exec_():\n",
    "            file_names = file_dialog.selectedFiles()\n",
    "            self.file_label.setText('\\n'.join(file_names))\n",
    "            self.table.setRowCount(0)\n",
    "            reference_seq = self.search_input.text().replace(\"\\n\", \"\")  \n",
    "            padding = 15\n",
    "            for file_name in file_names:\n",
    "                with pdfplumber.open(file_name) as pdf: \n",
    "                    whole_text, page_number = get_loaded_pdf(pdf) # Получаем текст из пдф reference_seq\n",
    "                    df = get_nearest_similarities(whole_text, reference_seq) # создаём df со всеми cos similiarity\n",
    "                    final_df = get_final_df(df, reference_seq, page_number, whole_text) # выполняем преобразования по дф\n",
    "                    for index, row in final_df.iterrows():\n",
    "                        left_b = max(0, int(row[\"index\"])-padding)\n",
    "                        left_text = whole_text[left_b: row[\"index\"]] + \"|\" \n",
    "                        res_text_ = whole_text[row[\"index\"]:row[\"index\"]+len(reference_seq)+1]\n",
    "                        right_b = row[\"index\"]+padding+len(reference_seq)\n",
    "                        right_text = \"|\" + whole_text[row[\"index\"]+len(reference_seq) + 1:right_b]\n",
    "                        res_text_ = left_text + res_text_ + right_text \n",
    "                        #print(left_b, right_b, whole_text)\n",
    "                        res = find_difference(reference_seq, row[\"texts\"])\n",
    "                        self.table.insertRow(self.table.rowCount())\n",
    "                        self.table.setItem(self.table.rowCount() - 1, 0, QTableWidgetItem(file_name))\n",
    "                        self.table.setItem(self.table.rowCount() - 1, 1, QTableWidgetItem(str(row[\"end_page_id\"])))\n",
    "                        self.table.setItem(self.table.rowCount() - 1, 2, QTableWidgetItem(res))\n",
    "                        self.table.setItem(self.table.rowCount() - 1, 3, QTableWidgetItem(reference_seq))  \n",
    "                        self.table.setItem(self.table.rowCount() - 1, 4, QTableWidgetItem(res_text_)) \n",
    "                        self.table.setItem(self.table.rowCount() - 1, 5, QTableWidgetItem(row[\"mask\"])) \n",
    "\n",
    "    def remove_files(self):\n",
    "        self.file_label.clear()\n",
    "        self.table.setRowCount(0)\n",
    "\n",
    "    def check_files(self):\n",
    "        self.table.setRowCount(0)\n",
    "        self.choose_files()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication([])\n",
    "    widget = MyWidget()\n",
    "    widget.show()\n",
    "    app.exec_()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c97413",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01536062",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
